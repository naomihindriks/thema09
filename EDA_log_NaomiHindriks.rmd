---
title: "Thema 9 Log: Breast Cancer Wisconsin (Original) Data Set"
author: "Naomi Hindriks"
date: "9/21/2021"
output:
  bookdown::pdf_document2:
    toc: false
    number_sections: false
urlcolor: blue
header-includes:
 \usepackage{float}
 \floatplacement{figure}{H}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(kableExtra)
library(dplyr)
library(ggplot2)
library(ggpubr)
library(scales)
library(ggfortify)
library(ggalt)
library(ggcorrplot)
library(forcats)
library(caret)
library(patchwork)
```

## EDA : Breast Cancer Wisconsin (Original) Data Set

### Data description

The data set: **Breast Cancer Wisconsin (Original) Data Set** is downloaded from the [UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29). The data were collected by the University of Wisconsin Hospitals, Madison by Dr. William H. Wolberg.

The UCI website states that the data set contains 699 instances. According to the corresponding *breast-cancer-wisconsin.names* file (also downloaded from the UCI website) each instance is made up of 10 attributes, plus the class attribute. More detailed information of these attributes is shown in Table 1. The information found in Table 1 is a combination of information that was found in the *breast-cancer-wisconsin.names* file and [User Manual Breast Cancer Diagnosis Web User Interface](https://www.rai-light.com/docs/BCD_User_Manual_v01.pdf) that includes an explanation on how to score the cytological characteristic.

According to the *breast-cancer-wisconsin.names* file there are 16 instances that contain a single missing attribute value, these are represented by "?" characters in the data file. It also states that out of the 699 instances there are 458 (65.5%) classified as benign and 241 (34.5%) classified as malignant.

To ensure the continued availability of the data and names files, they were copied to a personal [repository](https://github.com/naomihindriks/thema09).


```{r load-attribute-info}
attribute.info <- read.csv("attribute_info.csv", sep=";") 

attribute.info.temp <- data.frame(
  "Column" = attribute.info$column, 
  "Attribute" = attribute.info$full.name,
  "Unit" = attribute.info$unit,
  "Description" = attribute.info$description
  )

kbl(
    attribute.info.temp, 
    row.names = F, 
    caption = "Attribute Information. The cytological characteristics of breast FNAs (seen in rows 2-10) get a score from 1 to 10 by an examining physician with 1 being the closest to benign and 10 the most anaplastic.", 
    booktabs = T, 
    linesep = "",
    longtable = T
  ) %>%
  kable_styling(latex_options = c("striped")) %>%
  column_spec(1:3, width = "1.5cm") %>%
  column_spec(4, width = "10cm")

#clean up environment
remove(attribute.info.temp)
```

### Data loading and prepping

```{r load-data}
data <- read.table(file = 'data/breast-cancer-wisconsin.data',
                                     header = F,
                                     sep = ",",
                                     na.strings = '?')

str(data)

```

The data has been loaded, but it can be seen that the column names were not included in the data file. Furthermore the data does not seem to be of the correct data type, columns 2-10 should all be (ordered) factors. 
To give the columns the correct names and have easy access to the column descriptions I have created a simple csv file (attribute_info.csv).

```{r change-data-names-types}
names(data) <- attribute.info$name

data$class <- factor(data$class, levels = c(2, 4), labels = c("Benign", "Malignant"))

for(col.name in names(data)[2:10]) {
  data[, col.name] <- factor(data[, col.name], levels=1:10, ordered=T)
}

str(data)

#clean up environment
remove(col.name)
```

Now the columns have names and the values are of the correct data type.

### Data verification

The original data description stated that 699 instances with 10 attributes + a class label are present.

```{r inspect-data-dim}
dim(data)
```

This checks out. The original data description also stated that there are 16 instances with a single missing value, the instances that have a mising value will be removed from the data set. This means there are no more than 16 missing values and the number of complete cases should be 699 - 16.

```{r inspect-missing-data}
sum(is.na(data))

complete.instances <- complete.cases(data)

699 - sum(complete.instances)
```

This is correct. According to the original data description the class distribution is as follows: benign: 458 (65.5%), malignant: 241 (34.5%).

```{r inspect-class-distribution}
summary(data$class)

format(summary(data$class) / nrow(data) * 100, digits = 3)
```
Again this checks out. The last thing that I am going to check are the *Sample code numbers* of instances. The data original description stated that this is an id number, therefore I assume all of these numbers should be unique.

```{r inspect-unique-data}
length(unique(data$id))
```
The number of unique *sample code numbers* is 645, which is less than the 699 instances in the data. This is odd and requires further investigation. According to the original data description the data set is divided in 8 different groups, each group being collected in a different period of time. The groups 1 to 8 contain 367, 70, 31, 17, 48, 49, 31 and 86 instances respectively. Perhaps the *sample code numbers* of the instances are unique within their group.

```{r inspect-duplicated-data-per-group}
group.sizes <- c(367, 70, 31, 17, 48, 49, 31, 86)
duplicates.per.group <- c()

current.slice.start <- 0
i <- 0

for (group.size in group.sizes) {
  i <- i + 1
  group.row.numbers <- (current.slice.start + 1):(current.slice.start + group.size)
  current.slice.start <- current.slice.start + group.size
  
  duplicates <- sum(duplicated(data$id[group.row.numbers]))
  duplicates.per.group <- c(duplicates.per.group, duplicates)
  
  print(paste("Group ", i, ": ", duplicates , sep = ""))
}

# The total of duplicates when only looking inside group
sum(duplicates.per.group)

# The total duplicates in and outside group
nrow(data) - length(unique(data$id))

#clean up environment
remove(group.sizes, duplicates.per.group, 
       current.slice.start, i, group.size, 
       group.row.numbers, duplicates)
```

When looking at these numbers it is clear that there are duplicates within the groups and duplicates between different groups. This means that it is not logical that the duplicates are just duplicated rows that somehow got copied an extra time, because if that were the case we would expect to see only duplicates within groups. It is also not logical that the *sample code numbers* are reused in different groups since there are also duplicates within the groups. The next step is to check if the instances with duplicated *sample code numbers* have every attribute duplicated.


```{r inspect-duplicated-rows}
nrow(data[duplicated(data), ])
```
There are 8 rows that are an exact copy of another row, this means that there are instances with the same *sample code number* but different values for the other attributes. Tables 2-47 show the instances that share their *sample code number* with at least one other instance. The tables that have duplicates where every attribute is the same have a red header. 

```{r print-duplicated-data-tables, results='asis'}
duplicated.ids <- unique(data$id[duplicated(data$id)])

for (duplicate.id in duplicated.ids) {
  duplicate.entries.temp <- data[data$id == duplicate.id, ]
  
  if (sum(duplicated(duplicate.entries.temp)) > 0) {
    header.color <- "red"
  } else {
    header.color <- "white"
  }
  
  table <- kbl(
    duplicate.entries.temp, 
    row.names = T, 
    col.names = attribute.info$full.name,
    caption = paste("Instances with duplicate id:", duplicate.id), 
    booktabs = T, 
    linesep = ""
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "HOLD_position")) %>%
  column_spec(1:11, width = "1.5cm") %>%
  row_spec(0, background = header.color)
  
  print(table)
}

#clean up environment
remove(duplicate.entries.temp, table, header.color, duplicated.ids, duplicate.id)
```

When inspecting these tables it becomes apparent that the duplicated data is sometimes in consecutive rows, but not always. It can also be observed that most of the instances with duplicated *sample code numbers*  have the same class label, but not always. You can also see that most duplicates come in pairs, but they also come in bigger groups, up to 6 instances in one group (see Table 14). I do not see any pattern in how these rows are duplicated, nor can I think of any logical explanation for this. Since I do not want to risk using instances that are from the same person or sample I will keep only one instance per *sample code number*, and remove the duplicated rows.


### Removing data

First the instances with a missing value will be removed. After that the instances with a duplicated *sample code number* will be removed.

```{r removing-instances-missing-data}
# Keep unfiltered data in variable
unfiltered.data <- data

# only keep rows with complete instances
data <- data[complete.instances, ]

# verify 16 instances have been removed
dim(data)

#clean up environment
remove(complete.instances)
```
After removing the instances with a missing value, there are 683 instances left, which is as expected because 699 - 16 = `r 699 - 16`

```{r removing-instances-duplicated-id}

# find instances with duplicated id
duplicates <- duplicated(data$id)

# remove duplicate instances from data
data <- data[!duplicates, ]

# Making the id the rowname and removing id column
row.names(data) <- data$id
data <- data[2:11]

# print what the dimensions are after cleaning the data
dim(data)
```

Then after removing the instances with duplicated *sample code numbers* there are 630 instances left, these instances do not have missing values or duplicated *sample code numbers*.

### Exploring variables

A first scan of the attributes.

```{r}
summary(data)
```

For all attributes (except the class attribute) the most common value is 1 or 2. This makes sense, the most instances are classified as benign and lower numbers indicate more benign characteristics. Now I will make a table and bargraph to compare the class distribution before and after the filtering.

```{r class-distribution-table}
class.distribution <- data.frame(
  filtered = c(rep("before", nrow(unfiltered.data)), rep("after", nrow(data))), 
  class = c(as.character(unfiltered.data$class), as.character(data$class)))


d1 <-  class.distribution %>% group_by(filtered, class) %>%             
  tally %>% 
  bind_rows(class.distribution %>% group_by(filtered) %>%
      tally %>%
      mutate(class="Total")) %>%
  mutate(pct = round(n/((sum(n)/2))*100)) %>%
  arrange(desc(filtered))

kbl(
  d1[,2:4],
  caption = "Data distribution before and after filtering, n is the number of instances and pct is the percentage of instances.") %>%
  kable_styling(latex_options = c("HOLD_position")) %>%
  pack_rows(index = table(fct_inorder(d1$filtered)))

```

```{r class-distribution-bargraph, fig.cap="Distribution of the class attribute of the data before and after the filtering steps (the filtering steps are: removing rows with missing data, and than removing duplicated sample code numbers). The numbers in the bars are the actual number of instances in the data set."}
ggplot(class.distribution, aes_string(x = "class", y = "..prop..")) +
  geom_bar(
    aes(fill = factor(filtered), group = -as.numeric(factor(filtered))), 
      position = position_dodge()
      ) + 
  geom_text(
    aes(label = ..count.., group = -as.numeric(factor(filtered))), 
    stat = "count", 
    position = position_dodge(width = 0.9),
    vjust = 2) +
  scale_y_continuous(labels=scales::percent) +
  scale_fill_manual(
      name = "Data set", 
      values = c(hue_pal()(2)), 
      breaks = c("before", "after"), 
      labels = c("Data before filtering", "Data after filtering")) +
  labs(title="Class distribution before and after filtering of the data set") +
  xlab("Class") +
  ylab("Pecentage of data set")
```


When looking at *Table \@ref(tab:class-distribution-table)* and *Figure \@ref(fig:class-distribution-bargraph)* you can see that the class distribution hasn't changed all that much from the filtering steps. This is a positive sign, if it had changed a lot the distribution after filtering might not have been representative anymore. Luckily that is not the case.

I will make bar plots to show the distribution of the cytological characteristic. I chose bar plots for this because the data is ordinal.


```{r attr-distribution-barplot-old, fig.height=12, fig.width=10, fig.cap="Distribution of data in percentage for 9 different cytological characteristics for benign instances, malignant instances and for all instances together", include=F}
long.data <- pivot_longer(data, 1:9)

names.labs <- attribute.info$full.name
names(names.labs) <- attribute.info$name

ggplot(long.data, aes_string(x = 'value', y = "..prop..", group = 1)) +
  geom_bar(aes(fill="Both classes")) +
  geom_bar(
    aes(fill = class, group = class), 
    position = position_dodge2(preserve = "single"), 
    alpha = 0.8
    ) +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_fill_manual(
    name = "Class", 
    values = c("grey50", hue_pal()(2)), 
    breaks = c("Both classes", "Benign", "Malignant"), 
    labels = c("Both classes", "Benign", "Malignant")
  ) +
  labs(
    title="Distribution of cytological characteristics scores",
    x ="Score on a scale from 1 to 10", 
    y = "Percentage of instances"
    ) +
  facet_wrap(. ~ name, scales = "free", ncol = 2, labeller = labeller(name = names.labs)) +
  theme(legend.position = c(0.75, 1/8), legend.direction = "horizontal")
  #    ) + 

```

```{r attr-distribution-barplot, fig.height=10, fig.width=8, fig.cap="Distribution of data in percentage for 9 different cytological characteristics for benign instances, malignant instances and for all instances together"}
long.data <- pivot_longer(data, 1:9)

names.labs <- attribute.info$full.name
names(names.labs) <- attribute.info$name

ggplot(long.data, aes(x=value)) +
  geom_bar(aes(y = ..prop.., fill = name, group = class), stat="count") +
  labs(y = "Percent", fill="Attribute") +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_discrete(
    name = "Attribute", 
    breaks = sort(attribute.info$name),
    labels = attribute.info$full.name[order(attribute.info$name)]
    ) +
  labs(
    title="Distribution of cytological characteristics scores",
    x ="Score on a scale from 1 to 10", 
    y = "Percentage of instances"
    ) +
  facet_grid(
    name ~ class, 
    scales = "free", 
    margin = "class", 
    labeller = labeller(name = names.labs)
  ) +
  theme(legend.position = "bottom", strip.text.y = element_blank())

```


```{r, fig.height=12, fig.width=10, include=F}
df <- data

for(i in 1:9) {
  df[,i] <- as.numeric(df[,i])
}

long.df <- pivot_longer(df, 1:9)

long.df2 <- long.df %>%
  mutate(class_ = factor(class))

ggplot(long.df2, aes(x=value)) +
  geom_bar(aes(y = ..prop.., fill = class), stat="count") +
  labs(y = "Percent", fill="Class") +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(name ~ class_, scales = "free", margin = "class_") +
  theme(legend.position = "bottom")
```

\newpage

When looking at the distribution in *Figure \@ref(fig:attr-distribution-barplot)* it seems there is a difference in distribution between the benign and malignant samples for every attribute. When looking at the overall distributions for the attributes it is important to keep in mind that the benign samples have a bigger influence on this because there are more instances with that class. The seemingly big difference in distributions for all of these attributes is a positive sign for machine learning, all of these attributes could be useful in differentiating benign and malignant samples from one another. 


Now I will test if the differences observed in *Figure \@ref(fig:attr-distribution-barplot)* are significant. I will use the Mann–Whitney U test for this as this is a non-parametric test that can be used for data that do not follow a normal distribution and ordinal data. For each attribute I will test if the malignant samples are significantly greater than the benign samples.


```{r mann-whitney-tests}
temp <- data.frame("attribute" = character(), "estimate" = double(), "pval" = double())

temp.data <- data

temp.data$class <- relevel(temp.data$class, ref = "Malignant")

for(attr in colnames(temp.data)[1:9]) {
  res <- wilcox.test(
    as.numeric(temp.data[,attr]) ~ temp.data$class, 
    conf.int = TRUE, 
    alternative = "greater"
  )
  
  full.name <- attribute.info$full.name[attribute.info$name == attr]
  temp <- rbind(temp, data.frame("attribute" = full.name, 
                                 "estimate" = res$estimate,
                                 "pval" = res$p.value))
}

row.names(temp) <- NULL;

kbl(
  temp, 
  col.names = c("Attribute name", "Estimate median of difference", "P value"),
  booktabs = T,
  digits = c(100),
  linesep = "",
  caption = "Results of one-sided Mann–Whitney U test for each attribute where the null hypothesis is that the distribution of the malignant samples \\textbf{is not} higher than that of the benign samples. And the alterernative hypothesis is that the distribution of the Malignant samples \\textbf{is} higher than that of the benign samples. All of the p-values are well below 0.05 so we reject the null hypothesis for each attribute."
) %>% 
  kable_styling(latex_options = c("HOLD_position", "striped"))

remove(temp, temp.data)
```


In *Table \@ref(tab:mann-whitney-tests)* the results of one-sided Mann-Whitney U tests are displayed. Each row of the table represents a Mann-Whitney U tests on an attribute. The null hypothesis for each test is that the distribution of malignant samples **is not** shifted to the right compared to the distribution of the benign samples. The alternative hypothesis is that the distribution of malignant samples **is** shifted to the right compared to the distribution of the benign samples. If the p-value for one such test is lower than 0.05 the null hypothesis is rejected. Since all the p-values are well below 0.05 the null hypothesis is rejected in all the tests. The estimate median of difference is the median of the difference between all the instances in the two samples. The size of the estimate median of difference of the bare nuclei is the largest. This is logical when looking at *Figure \@ref(fig:attr-distribution-barplot)* you can see that most of the benign samples have a score of 1 and most of the malignant samples have a score of 10. The estimate median of difference of the mitosis also stands out. This estimate is very small, close to zero even. This also makes sense when looking at the *Figure \@ref(fig:attr-distribution-barplot)* as you can see that for both benign and malignant cases most instances have a score of 1, but in the malignant cases the data is spread out more. So even though the change is not big there is a significant difference in distribution of mitosis between benign and malignant samples.


### Relation between attributes

Now I will look at the correlation between the different attributes. My expectation is that some or all of the attributes will be positively correlated because the scores of the attributes range from 1 to 10, with 1 being the closest to benign and 10 the most anaplastic. This means that if the sample is benign, all or most of the scores will be 1 or close to 1, and if the sample is malignant the scores will tend to be higher than 1. This idea is also supported by the fact that for all the attributes the distributions of the malignant samples were significantly shifted to the right compared to the benign distributions, they all shift in the same direction. For calculating the correlation score I will not use the Pearson method, as it makes the assumption that the data is normally distributed, in this case the data is not normally distributed, so I will have to find a different method. I have chosen to use Kendall $\tau_{b}$ rank correlation as seems to be better at handling ties.


```{r attr-correlation-plot, fig.cap="Correlations between the attributes"}
df <- data

for(i in 1:9) {
  df[,i] <- as.numeric(df[,i])
}
colnames(df) <- attribute.info$full.name[-1]

# Calculate  p values for correlation coefficients
correlation.p.values.kendall <- cor_pmat(df[,1:9], method = "kendall")

# Plot correlation coefficients for attributes
ggcorrplot(
  cor(df[,1:9], method = "kendall"),
  type = "lower", 
  outline.col = "white",
  lab = TRUE,
  p.mat = correlation.p.values.kendall
) +
  ggtitle("Correlation between the attributes")
```


```{r attr-correlation-p-value}
# Print p values in table
kbl(
  correlation.p.values.kendall,
  booktabs = T, 
  linesep = "",
  caption = "P values of Kendall $\\tau_{b}$ correlation coefficient",
  digits = 50,
  escape = F
) %>%
  column_spec(1:10, width = "3cm") %>%
  column_spec(1:10, width = "2cm") %>%
  kable_styling(latex_options = c("HOLD_position", "striped", "scale_down"))
```

When looking at the correlations between the variables in *Figure \@ref(fig:attr-correlation-plot)* it is clear that most of the attributes have a moderate to strong relationship. Clump thickness and mitosis seem to have a weak to moderate relationship to the other attributes. The attributes uniformity of cell size and uniformity of cell shape stand out as having a very strong relationship with each other (with a Kendall's $\tau_{b}$ correlation coefficient of 0.82). As expected all the correlations are positive. When looking at the p-values corresponding to the correlation coefficients in *Table \@ref(tab:attr-correlation-p-value)* it can be seen that the p-values are all very small meaning that all the correlation coefficients in *Figure \@ref(fig:attr-correlation-plot)* are statistically significant. These correlation coefficients could impact specific classifier learning algorithms. One such example is the Naive Bayes classifier, as it assumes that the values of the attributes are independent of each other.

### Principal component analysis

Next I will conduct principal component analysis, so we can see if two distinct groups can be seen based on the principal components. I have some doubts whether or not I should scale the data before performing the PCA. All of the attributes are on a scale from 1 to 10. Before making the decision I will check the variance of the different attributes.

```{r variance-per-attr}
vars <- apply(data[c(-ncol(data))], 2, var)
attr.names <- attribute.info[attribute.info$name %in% names(vars),][,c("full.name", "name")]

var.per.attr.df = data.frame(attr.name = attr.names$full.name, vars = vars[attr.names$name])

kbl(
  var.per.attr.df,
  caption = "Variance per attribute",
  row.names = F,
  col.names = c("Attribute", "Variance"),
  booktabs = T,
  linesep = "",
  digits = 3) %>%
  kable_styling(latex_options = c("striped", "HOLD_position")) %>%
  column_spec(1:2, width = "7cm")
```

The variance of the attributes is displayed in *Table \@ref(tab:variance-per-attr)*. The variance is not the same for the attributes. This means that when performing PCA that the attribute with the highest variance will have the most influence on the principal components, and the attributes with the lowest variance will only have a little influence on the principal components. The question remains whether the attributes values are on the same scale. All the attributes are on an ordinal scale from 1 to 10, in this [manual](https://www.rai-light.com/docs/BCD_User_Manual_v01.pdf) you can find a more extensive explanation how these are scored. When reading the table on page 4 of this document it becomes clear that the attributes all are scored on a scale from 100% normal to 100% anaplastic, but the steps between the percentages differ slightly (most steps of all the attributes are 10% difference, but some include steps of 15%, and some have 1 step of 20%). It is also hard to say if 10% difference in mitosis means the same as 10% difference in clump thickness for example. Another problem for a correct PCA is that not all the steps are of the size, in an ideal PCA every step would have the same meaning. 

Now I will do the analysis. I have chosen to do the analysis twice, once with scaling and once without scaling and compare the results with each other.


```{r scree-pca-plots, fig.height=10, fig.width=10, fig.cap="Principal component analysis"}
scaled_options = c(TRUE, FALSE)

pca.list = list()
plot.list = list()

for(scaled_option in  scaled_options) {
  # Get principal components
  pca.res <- prcomp(df[1:9], scale. = scaled_option, center = TRUE)
  pca.list[[paste("scaled.", scaled_option, sep = "")]] <- pca.res
  
  # Calculate explained variance
  var.explained.df <- data.frame(
    PC= paste0("PC",1:9), 
    var.explained=(pca.res$sdev)^2/sum((pca.res$sdev)^2)
  )

  # Plot explained variance for PC's
  new.scree.plot <- ggplot(var.explained.df, aes(x=PC,y=var.explained, group=1))+
    geom_point(size=4)+
    geom_line()+
    ylab("Variance explained") +
    xlab("Principal component")
  
  # Get points to plot in PCA plot
  df.pca <- data.frame(pca.res$x, class=data$class)
  df.benign <- df.pca[df.pca$class == "Benign", ] 
  df.malignant <- df.pca[df.pca$class == "Malignant", ]
  
  # PCA plot
  new.pca.plot <- ggplot(df.pca, aes(PC1, PC2, col=class)) + 
    geom_point() +
    coord_cartesian(xlim = 1.2 * c(min(df.pca$PC1), max(df.pca$PC1)), 
                    ylim = 1.2 * c(min(df.pca$PC2), max(df.pca$PC2))) +
    geom_encircle(data = df.benign) +
    geom_encircle(data = df.malignant) +
    xlab("Principal component 1") + 
    ylab("Principal component 2") + 
    theme(legend.direction = "horizontal", legend.background = element_rect(linetype = "solid", size = 1))
  
  leg <- get_legend(new.pca.plot)
  new.pca.plot <- new.pca.plot + theme(legend.position = "none")
  
  plot.list[[paste("scree.plot.scaled.", scaled_option, sep = "")]] <- new.scree.plot
  
  plot.list[[paste("pca.plot.scaled.", scaled_option, sep = "")]] <- new.pca.plot

}

# Add legend to plotlist
plot.list[["leg"]] <- leg

# Titles for rows and columns wrapped plot
row1 <- ggplot() + 
  annotate(
    geom = 'text', 
    x=1, y=1, 
    label="Scaled = TRUE", 
    angle = 90, 
    size = 5,
    fontface = 2) + 
  theme_void() 
row2 <- ggplot() + 
  annotate(
    geom = 'text', 
    x=1, y=1, 
    label="Scaled = FALSE", 
    angle = 90, 
    size = 5,
    fontface = 2) + 
  theme_void()
col1 <- ggplot() + 
  annotate(
    geom = 'text', 
    x=1, y=1, 
    label="Explained variance", 
    size = 5.5,
    fontface = 2) + 
  theme_void() 
col2 <- ggplot() + 
  annotate(
    geom = 'text', 
    x=1, y=1, 
    label="PCA plot", 
    size = 5.5,
    fontface = 2) + 
  theme_void() 

title.list <- list(a = row1, b = row2, e = col1, f = col2)

layoutplot <- "
#ccccdddd
aeeeeffff
aeeeeffff
aeeeeffff
bgggghhhh
bgggghhhh
bgggghhhh
#####oooo
"

wrap_plots(plotlist = c(title.list, plot.list), guides = 'collect', design = layoutplot)
```


```{r table-principal-components-rotations-scaled}
kbl(
    pca.list[["scaled.TRUE"]]$rotation,
    caption = "PCA: Rotation matrix of the 9 cytological characteristics to each principal component, scaled = TRUE", 
    booktabs = T, 
    linesep = ""
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "HOLD_position")) %>%
  column_spec(1:9, width = "2cm")
```

```{r table-principal-components-rotations-not-scaled}
kbl(
    pca.list[["scaled.FALSE"]]$rotation,
    caption = "PCA: Rotation matrix of the 9 cytological characteristics to each principal component, scaled = FALSE", 
    booktabs = T, 
    linesep = ""
  ) %>%
  kable_styling(latex_options = c("striped", "scale_down", "HOLD_position")) %>%
  column_spec(1:9, width = "2cm")
```

Now it is time for the fun part: interpreting the results of the PCA's. In the PCA plots in *Figure \@ref(fig:scree-pca-plots)* it can be seen that with or without scaling, the benign and malignant samples clearly form two distinct clusters, this is a positive sign for making a classifier for this data. The in the unscaled plot the clusters might seem a bit more distinct than in the scaled plot.

In the scaled PCA plot (top right) the variance in the second principal component seems to represent a small amount of instances. This second component does seem to say something about malignancy, you can see that only 3 benign instances are higher than -2, a lot more malignant instances are in this area (even though benign is the majority class). Looking at *Table \@ref(tab:table-principal-components-rotations-scaled)* we can observe that the second component is dominated by the mitosis attribute. Comparing the second principle component to the distribution of mitosis in *Figure \@ref(fig:attr-distribution-barplot)* the distribution of the second component makes sense, most of the instances have a mitosis of 1, but from the instances that have a mitosis score higher than 1 most of them are malignant.

In the unscaled PCA plot (bottom right) principal component 1 seems to have a dividing line between benign and malignant at around -2.5. On principal component 2 the benign instances seem to be concentrated around 0 and the malignant instances are more dispersed.

Comparing the explained variance plots in *Figure \@ref(fig:scree-pca-plots)* you can see that they are quite the same. In both cases the first component explains by far most of the variance. In the unscaled PCA the first component explains a little more of the variance than the scaled version and the second component a little less. The difference in these plots doesn't seem significant. It is important to keep in mind that in the unscaled version the different attribute do not contribute equally to the total variance.

When comparing *Table \@ref(tab:table-principal-components-rotations-scaled)* and *Table \@ref(tab:table-principal-components-rotations-not-scaled)* you can see that in *Table \@ref(tab:table-principal-components-rotations-scaled)*, the scaled PCA, the attributes contribute quite equally to principal component 1, the smallest contributor to component 1 is mitosis, but mitosis does dominate the second component. This is explained by mitosis having a smaller relationship to all the other attributes, as can be seen in the correlation plot (the greater the correlation between two attributes, the more they can be displayed on the same axis). In *Table \@ref(tab:table-principal-components-rotations-not-scaled)*, the unscaled PCA, the attribute with the biggest variance (bare nuclei) contributes most to the first as well as the second principal component and mitosis does not play a big role in either of the two. 

Keeping the correlations between the attributes in mind, and the fact that all the attributes contribute more equally I think that the PCA plot with scaling is the better visualization of the multidimensional data, but the unscaled plot seems to have a better division between benign and malignant instances, this could be caused by the fact that the attribute with the biggest variance is also the best predictor of malignancy. However this is not the core purpose of PCA. If we want to find the axis that represents the biggest separation in class we should do *linear discriminant analysis*.

\newpage

## Machine Learning Phase
